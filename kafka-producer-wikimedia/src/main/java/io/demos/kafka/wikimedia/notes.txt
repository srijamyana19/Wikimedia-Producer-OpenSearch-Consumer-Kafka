So you have different settings and this is a broker setting.
So you can set compression.type=producer and it's a default.
That means that the broker is going to take the compressed batch from the producer clients and is going to write it directly to the topic's log file without recompressing the data.
So it is optimal, but that pushes the necessity of compression onto the producer.
You can also have compression.type=none.
In which case, all the batches sent to Apache Kafka are going to be decompressed by the broker which I think is a bit inefficient, but why not?
You can, for example set a specific type of compression for the settings.
For example, compression.type=lz4 and then some interesting behavior happens.
So if the compression type set on the topic is equal to the one on the producer setting then the data is not going to be recompressed.
It's just going to be stored on disk as is.
But if you're using a different compression mechanism on the producer side, then the batch is going to be first decompressed by the broker.
And then recompressed using the compression algorithm specified for example lz4 in this example.
So just so you know, if you enable broker-side compression is going to consume extra CPU cycles.
So overall my best recommendation would be for you to make sure that all your producers are compressing the data on their end and leave the broker default to compression.type=producer.
But in case you have no control over your producers but you still want to enable compression then you can enable it on the broker side.

So, batching is a good thing in Apache Kafka because it helps improve throughput and compression, and therefore, there are two settings that you can use to influence the batching mechanism.
The first one is "linger.millisecond".
The default value is zero, which is how long to wait until we send a batch.
And for example, if you said this to five milliseconds, then you introduce a small delay, a small latency of five milliseconds but your Kafka producer is going to wait up to five millisecond to add more messages in the batch before sending it.
And the batch size is also saying that if a batch is filled up before the "linger.millisecond" has been achieved, then send the batch.
And so we can increase the batch size if we want larger batch sizes.
properties.setProperty(ProducerConfig.COMPRESSION_TYPE_CONFIG, "snappy"); //Snappy, I like it because it has a good balance of CPU to compression ratio,
properties.setProperty(ProducerConfig.LINGER_MS_CONFIG, "20");
properties.setProperty(ProducerConfig.BATCH_SIZE_CONFIG, Integer.toString(32*1024));

So when your key is not null then your data is gonna go through a partitioner logic which decides how a record gets assigned to a partition and this process is called Key Hashing, which is the process of determining the mapping of a key to a partition.
And in the default Kafka partitioner the key are hashed using the murmur2 algorithm and this is the formula.
targetPartition = Math.abs(Utils.murmur2(keyBytes))%(numPartitions-1)
So the default producer has two behaviors.
Number one, up to Kafka 2.3 we have the Round Robin behavior and for Kafka 2.4 and above we have the Sticky partitioner
The idea is that when we use the sticky partitioner we're going to get a huge performance improvement especially when you have, with high throughputs and your key is null.

max.block.ms and buffer.memory.
So if the producer becomes very, very, very high throughputs and the broker cannot respond fast enough to these requests, the records are going to be buffered in memory on your producer.
And the buffer size of buffer.memory is 32 megabytes.
It is the size of the send buffer.
And that buffer is going to fill up over time when the broker is too busy.
And then if the broker becomes less busy, it's going to empty back down when the throughputs at the broker increases.
So the idea is that this buffer is here to queue up messages before sending them into Kafka.

Now we can obviously increase the buffer memory if we fill it up too much.
So if the buffer is full, all the 32 megabytes of it, then next time you do .send() on your producer, it will start to block.
That means that it will block at a send of code.
It will not be a synchronous anymore.
It will not return right away.
It will just block your code to prevent the buffer from filling up.
And then goes a new setting in place.
So if the send method is blocking, then max.block.ms is going to be 60,000.
That means that for 60 seconds, it's okay to be blocked on send.
If the buffer is not unblocked after all this time.
So if the buffer is still not emptied out after we have sent the message, then the send message is going to throw an exception.
So either the producer has fill up his buffer and the broker is not accepting new data and 60 seconds has elapsed.
And if you hit an exception that means that your brokers are down or really overloaded and they cannot respond to any types of request.

So you have a consumer group application with three consumers and the consumers in a Consumer Group they talk to something called a Consumer Group Coordinator.
And it's an acting broker and this is used to detect whether or not your consumers are still up.
So there is a heartbeat mechanism, okay?
And there's a poll mechanism.
So the heartbeat thread is going to be your consumers sending messages to the broker once in a while saying they're still alive, okay?
And the poll thread is going to be other brokers thinking your consumers are still alive because they are still requesting data from Apache Kafka.
So these two mechanisms together capture a lot of issues that can happen with your consumers and so we'll have a look at these in details, okay?
But overall, it is very encouraged for you to process data fast and pull often versus the opposites.
So let's talk about the consumer heartbeat thread.
So the heartbeat thread sends data to Kafka, just a heartbeat, every once in a while to tell the consumer is alive.
And by default, the interval of the heartbeat is three seconds.
So you can control it and it says how often to send heartbeats and usually you set it to one third of session.timeout.ms. Session.timeout.ms by default is 45 seconds in Kafka 3.0 and before it was 10 seconds.
And so the idea is that the heartbeats are sent to the broker periodically and then if no heartbeat is sent during the timeout millisecond, then the consumer is considered dead, okay? So you would set session.timeout.ms to something really low for faster consumer rebalances in the cases where your consumer are exiting the group unexpectedly and they stop sending heartbeats, okay?
So this mechanism, the heartbeat thread, is used to detect the consumer application being done.
So if you want to have a consumer being killed and then the group to rebalance very quickly,
I would set heartbeat for example, to one second and I would set session.timeout.ms to say, four seconds, for example and it would work.
So that's one mechanism and the other one is the poll thread.
So we have the max.poll.interval.ms, which is by default five minutes, which is how long it's allowed between two poll before thinking that the consumer is dead.
And this is very relevant for example, when you have a big data framework that uses a consumer for example, Spark and the processing takes time.
So if your processing takes more than five minutes, then Kafka is going to think that your consumer is dead.
So this is used to detect whether or not there is a data processing issue and for example, the consumer becomes stuck in the processing.
So tweak for your needs.
If it's a very fast application, maybe you want to set this max.poll.interval.ms to 20 seconds but if it's a very slow application, maybe you want 10 minutes, I don't know.
Then you have max.poll.records, which is how many records you poll at a time.
So per poll request and so if your messages are very small, you can of course increase it but if your messages are very big, you need to decrease it because it may take you too much time to process the records.
So it's good for you to check it out how many records are being pulled per request and how big are your records and how long it takes you to process these records.
Next there is fetch.min.bytes by default one, which is how much data you want to pull at least from Kafka on each request.
And if you increase this, it helps improve throughput by decreasing the request number at the cost of latency because you're saying, hey, at least give me a megabyte of data before returning data to my consumer, otherwise I don't need it.
And fetch.max.wait.ms by default is half a second, which is the maximum amount of time the Kafka broker will block before answering the fetch request if there are not enough bytes to fulfill in the fetch.min.bytes, okay?
So that means that if you search, for example, fetch.min.bytes to one megabytes, then even if one megabyte is not here, it will take at most 500 milliseconds of latency before the fetch request is returned to the consumer.
So all these settings can help you really like tweak your consumer behavior.
Defaults are fine but if you ever get to this stage then this lecture should help.
Next we have max.partition.fetch.bytes, by default one megabyte, which is the maximum amount of data per partition that the server will return and if you read from 100 partitions, that mean you'll need at least 100 megabytes of RAM so adjust it based on what you need.
And fetch.max.bytes, which is the maximum amount of data returned for each fetch request and if you have available memory, then increase it to allow your consumer to read more data in each request.
So these are advanced settings and only to be modified if your consumer is maxing out on throughput and you want to improve it, okay?